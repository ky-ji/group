@inproceedings{ji2026testtime,
  selected={true},
  title = {Test-time Sparsity for Extreme Fast Action Diffusion},
  author = {Kangye Ji and Jianbo Zhou and Yuan Meng and Ye Li and Chen Tang and Zhi Wang},
  year = {2026},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pdf = {https://openreview.net/pdf?id=e2rXzov882},
  preview={SAG2.png},
  description={A test-time sparsity method for accelerating action diffusion models to achieve extreme fast inference.},
  keywords={Diffusion Policy, Sparsity, Embodied AI, Action Generation}
}

@inproceedings{li2026spvla,
  selected={true},
  title = {SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration},
  author = {Ye Li and Yuan Meng and Zewen Sun and Kangye Ji and Chen Tang and Jiajun Fan and Xinzhu Ma and Shutao Xia and Zhi Wang and Wenwu Zhu},
  year = {2026},
  booktitle = {International Conference on Learning Representations (ICLR)},
  pdf = {https://arxiv.org/pdf/2506.12723},
  preview={SP-VLA.png},
  description={A joint model scheduling and token pruning approach for accelerating Vision-Language-Action models.},
  keywords={VLA, Token Pruning, Model Scheduling, Efficient Inference}
}

@inproceedings{ji2026bac,
  selected={true},
  title = {Block-wise Adaptive Caching for Accelerating Diffusion Policy},
  author = {Kangye Ji and Yuan Meng and Hanyun Cui and Ye Li and Jianbo Zhou and Shengjia Hua and Lei Chen and Zhi Wang},
  year = {2026},
  booktitle = {International Conference on Learning Representations (ICLR)},
  pdf = {https://arxiv.org/pdf/2506.13456},
  code = {https://github.com/ky-ji/BAC.git},
  preview={BAC.png},
  description={A block-wise adaptive caching approach for accelerating diffusion policy inference.},
  keywords={Diffusion Policy, Caching, Embodied AI, Efficient Inference}
}

@article{chen2026wisparse,
  title = {WiSparse: Boosting LLM Inference Efficiency with Weight-Aware Mixed Activation Sparsity},
  author = {Lei Chen and Yuan Meng and Xiaoyu Zhan and Zhi Wang and Wenwu Zhu},
  year = {2026},
  journal = {arXiv preprint arXiv:2602.14452},
  pdf = {https://arxiv.org/pdf/2602.14452.pdf},
  preview = {Wi.png},
  description = {Weight-aware mixed-granularity activation sparsity for training-free LLM acceleration with adaptive sparsity allocation across blocks.},
  keywords = {LLM, Activation Sparsity, Efficient Inference}
}

@article{li2025prance,
  selected={true},
  title={Prance: Joint token-optimization and structural channel-pruning for adaptive vit inference},
  author={Li, Ye and Tang, Chen and Meng, Yuan and Fan, Jiajun and Chai, Zenghao and Ma, Xinzhu and Wang, Zhi and Zhu, Wenwu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  year={2025},
  pdf = {https://arxiv.org/abs/2407.05010},
  publisher={IEEE},
  preview={Prance.png},
  description={A joint token-optimization and structural channel-pruning approach for adaptive vit inference.},
  code={https://github.com/ChildTang/PRANCE},
  keywords={Vision Transformer, Token Pruning, Structural Channel Pruning, Efficient Inference}
}

@article{li2025ts,
  title={TS-DP: Reinforcement Speculative Decoding For Temporal Adaptive Diffusion Policy Acceleration},
  author={Li, Ye and Feng, Jiahe and Meng, Yuan and Ji, Kangye and Tang, Chen and Wen, Xinwan and Xia, Shutao and Wang, Zhi and Zhu, Wenwu},
  journal={arXiv preprint},
  year={2025},
  preview={TS-DP.png},
  pdf={https://arxiv.org/pdf/2512.15773},
  description={A reinforcement speculative decoding method for temporal adaptive diffusion policy acceleration.},
  keywords={Diffusion Policy, Reinforcement Learning, Speculative Decoding, Efficient Inference}
}

@article{ji2025sparseactiongen,
  title = {Sparse ActionGen: Accelerating Diffusion Policy with Real-time Pruning},
  author = {Kangye Ji and Yuan Meng and Jianbo Zhou and Ye Li and Hanyun Cui and Zhi Wang},
  year = {2025},
  journal = {arXiv preprint},
  pdf = {https://arxiv.org/pdf/2601.12894},
  preview={SAG.png},
  description={A real-time pruning method for accelerating diffusion policy in embodied AI.},
  keywords={Diffusion Policy, Pruning, Embodied AI, Action Generation}
}

@article{liu2025spatialpolicy,
  title = {Spatial Policy: Guiding Visuomotor Robotic Manipulation with Spatial-Aware Modeling and Reasoning},
  author = {Yijun Liu and Yuwei Liu and Yuan Meng and Jieheng Zhang and Yuwei Zhou and Ye Li and Jiacheng Jiang and Kangye Ji and Shijia Ge and Zhi Wang and Wenwu Zhu},
  year = {2025},
  journal = {arXiv preprint},
  pdf = {https://arxiv.org/pdf/2508.15874},
  preview = {spatial.png},
  description={A unified spatial-aware visuomotor robotic manipulation framework via explicit spatial modeling and reasoning, achieving over 33\% improvement on Meta-World and over 25\% improvement on iTHOR.},
  keywords={Robotics, Embodied AI, Spatial Reasoning, Visuomotor Control}
}

@inproceedings{jiang2025ood,
  title = {Quantization Meets OOD: Generalizable Quantization-aware Training from a Flatness Perspective},
  author = {Jiacheng Jiang and Yuan Meng and Chen Tang and Han Yu and Qun Li and Zhi Wang and Wenwu Zhu},
  booktitle = {ACM International Conference on Multimedia (ACM MM)},
  year = {2025},
  pages = {11937-11946},
  pdf = {https://dl.acm.org/doi/pdf/10.1145/3746027.3755856},
  code = {https://github.com/JachinJiang/Quantization-Meets-OOD-Generalizable-Quantization-aware-Training-from-a-Flatness-Perspective},
  preview = {ood.png},
  description = {A flatness-oriented QAT method (FQAT) to improve out-of-distribution generalization; introduces layer-wise freezing to mitigate gradient conflicts between QAT and flatness objectives.},
  keywords = {Quantization, QAT, OOD, Generalization}
}

@inproceedings{qdit2025,
  title = {Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers},
  author = {Lei Chen and Yuan Meng and Chen Tang and Xinzhu Ma and Jingyan Jiang and Xin Wang and Zhi Wang and Wenwu Zhu},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2025},
  pdf = {https://arxiv.org/pdf/2406.17343},
  code = {https://github.com/Juanerx/Q-DiT},
  preview = {q-dit.png},
  description = {Addresses large channel/dimension variance and step-dependent activation shifts in DiT quantization; proposes automatic granularity assignment and dynamic quantization. Achieves lossless W6A8 and about 20\% lower FID than prior SOTA.},
  keywords = {Diffusion Transformer, Post-Training Quantization, Efficient Inference}
}

@inproceedings{joint2025aaai,
  title = {Joint Automatic Architecture Design and Low-Bit Quantization with Hardware-Software Co-Exploration},
  author = {Mingzi Wang and Yuan Meng and Chen Tang and Weixiang Zhang and Yijian Qin and Yang Yao and Yingxin Li and Tongtong Feng and Xin Wang and Xun Guan and Zhi Wang and Wenwu Zhu},
  booktitle = {AAAI Conference on Artificial Intelligence (AAAI)},
  year = {2025},
  pdf = {https://arxiv.org/pdf/2501.05339},
  preview = {aaai2025.png},
  description = {Jointly optimizes network architecture, ultra-low mixed precision, and accelerator design; channel-level sparse quantization reduces memory, and hardware-generated networks accelerate search.},
  keywords = {Neural Architecture Search, Mixed Precision, Hardware-Software Co-Design}
}

@inproceedings{rfquant2024,
  title = {Retraining-free Model Quantization via One-Shot Weight-Coupling Learning},
  author = {Chen Tang and Yuan Meng and Jiacheng Jiang and Shuzhao Xie and Rongwei Lu and Xinzhu Ma and Zhi Wang and Wenwu Zhu},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2024},
  pdf = {https://arxiv.org/pdf/2401.01543.pdf},
  code = {https://github.com/1hunters/retraining-free-quantization},
  preview = {rf-quant.png},
  description = {Analyzes bit-width interference and introduces a bit-width scheduler with alignment to improve stability; achieves strong accuracy without retraining.},
  keywords = {Quantization, One-Shot, Efficient Inference}
}

@article{liu2024qllmgen,
  title = {Evaluating the Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox},
  author = {Yijun Liu and Yuan Meng and Fang Wu and Shenhao Peng and Hang Yao and Chaoyu Guan and Chen Tang and Xinzhu Ma and Zhi Wang and Wenwu Zhu},
  year = {2024},
  journal = {arXiv preprint arXiv:2406.12928},
  pdf = {https://arxiv.org/pdf/2406.12928},
  preview = {eva.png},
  description = {Provides a benchmark suite and toolbox to evaluate generalization of quantized LLMs across diverse datasets and calibration distributions.},
  keywords = {LLM, Quantization, Benchmark, Generalization}
}

@inproceedings{advrobust2024,
  title = {Investigating the Impact of Quantization on Adversarial Robustness},
  author = {Qun Li and Yuan Meng and Chen Tang and Jiacheng Jiang and Zhi Wang},
  booktitle = {ICLR PML4LRS Workshop},
  year = {2024},
  pdf = {https://arxiv.org/pdf/2404.05639.pdf},
  preview = {adv-robust.png},
  description = {Defines a quantization pipeline and decomposes components to analyze their effects on adversarial robustness.},
  keywords = {Quantization, Adversarial Robustness}
}

@article{li2024ems,
  title = {EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache Compression Based on Global-Local Importance},
  author = {Yingxin Li and Ye Li and Yuan Meng and Xinzhu Ma and Zihan Geng and Shutao Xia and Zhi Wang},
  journal = {arXiv preprint arXiv:2412.08521},
  year = {2024},
  pdf = {https://arxiv.org/pdf/2412.08521},
  preview = {EMS.png},
  description = {Proposes EMS with a Global-Local importance score and an adaptive Evict-then-Merge framework to improve head-wise KV cache compression, achieving strong performance under extreme compression ratios.},
  keywords = {LLM, KV Cache, Compression, Efficient Inference}
}

@misc{tmpqdm2024,
  title = {TMPQ-DM: Joint Timestep Reduction and Quantization Precision Selection for Efficient Diffusion Models},
  author = {Haojun Sun and Chen Tang and Zhi Wang and Yuan Meng and Jingyan Jiang and Xinzhu Ma and Wenwu Zhu},
  year = {2024},
  journal = {arXiv preprint},
  pdf = {https://arxiv.org/pdf/2404.09532.pdf},
  preview = {tmpqdm.png},
  description = {Achieves more than 10x BitOPs savings on five datasets while maintaining generation quality.},
  keywords = {Diffusion Model, Timestep Reduction, Quantization}
}

@inproceedings{seam2023,
  title = {SEAM: Searching Transferable Mixed-Precision Quantization Policy through Large Margin Regularization},
  author = {Chen Tang and Kai Ouyang and Zenghao Chai and Yunpeng Bai and Yuan Meng and Zhi Wang and Wenwu Zhu},
  booktitle = {ACM International Conference on Multimedia (ACM MM)},
  year = {2023},
  pdf = {https://arxiv.org/pdf/2302.06845.pdf},
  preview = {seam.png},
  description = {Searches transferable mixed-precision policies using large-margin regularization.},
  keywords = {Mixed Precision, Quantization Policy}
}

@inproceedings{elasticvit2023,
  title = {ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision Transformer on Diverse Mobile Devices},
  author = {Chen Tang and Li Lyna Zhang and Huiqiang Jiang and Jiahang Xu and Ting Cao and Quanlu Zhang and Yuqing Yang and Zhi Wang and Mao Yang},
  booktitle = {IEEE/CVF International Conference on Computer Vision (ICCV)},
  year = {2023},
  pdf = {https://arxiv.org/pdf/2303.09730.pdf},
  code = {https://github.com/microsoft/Moonlit/tree/main/ElasticViT},
  preview = {elasticvit.png},
  description = {Achieves up to 2x on-device inference speed across diverse mobile devices.},
  keywords = {Vision Transformer, Mobile Deployment, Efficient Inference}
}

@inproceedings{abn2022,
  title = {Arbitrary Bit-width Network: A Joint Layer-Wise Quantization and Adaptive Inference Approach},
  author = {Chen Tang and Haoyu Zhai and Kai Ouyang and Zhi Wang and Yifei Zhu and Wenwu Zhu},
  booktitle = {ACM International Conference on Multimedia (ACM MM)},
  year = {2022},
  pdf = {https://arxiv.org/pdf/2204.09992.pdf},
  preview = {abn.png},
  description = {Saves 10\%-15\% compute compared with highly compressed models.},
  keywords = {Quantization, Adaptive Inference}
}

@inproceedings{limpq2022,
  title = {Mixed-Precision Neural Network Quantization via Learned Layer-Wise Importance},
  author = {Chen Tang and Kai Ouyang and Zhi Wang and Yifei Zhu and Yaowei Wang and Wen Ji and Wenwu Zhu},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year = {2022},
  pdf = {https://arxiv.org/pdf/2203.08368.pdf},
  code = {https://github.com/1hunters/LIMPQ},
  preview = {limpq.png},
  description = {Layer-wise importance yields mixed-precision policy search up to about 300x faster.},
  keywords = {Mixed Precision, Quantization}
}
